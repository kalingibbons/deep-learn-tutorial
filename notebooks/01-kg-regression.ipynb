{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Regression with PyTorch\n",
    " This notebook will show you how to do a simple regression with PyTorch. We will use\n",
    " the  [California housing\n",
    " dataset](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset)\n",
    " to estimate the median house value in a given block based on a number of household\n",
    " features.\n",
    " > Note: Example pulled from [here](https://machinelearningmastery.com/building-a-regression-model-in-pytorch/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ##Handle our imports in one block for reproduciability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import copy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import tqdm.auto\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load our data\n",
    " Most machine learning frameworks include (cleaned) toy datasets for learning concepts and\n",
    " benchmarking. Here's how we can load the California housing dataset from scikit-learn.\n",
    " It can be used like a dictionary, but with dot-indexing as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_california_housing()\n",
    "_ = [print(f\"{k}\") for k in data.keys()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We're interested in the data and target values. The data is a 2D array of features,\n",
    " and the target is a 1D array of the median house value in a given block. Let's check\n",
    " their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Target: {data.target_names}\")\n",
    "print(f\"Features: {data.feature_names}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's get those into a pandas dataframe for easier inspection.\n",
    " > Note: the median house value is in units of 100,000 USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df[data.target_names[0]] = data.target\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Data Preparation\n",
    " We need to split our data into training and validation sets. We'll use 70% of the data\n",
    " for training and 30% for validation. We'll also standardize the data to have a mean of 0\n",
    " and a standard deviation of 1. This is a common preprocessing step for neural\n",
    " networks.\n",
    " Note: Scikit sticks to numpy arrays, with generic names, so we'll use those. You\n",
    " should also split your data into training, validation, and _test_ sets for real\n",
    " research. When you're tuning your hyperparameters, you're likely to overfit to your\n",
    " validation data, so you need to keep a separate test set to evaluate your model on\n",
    " after you've finished tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Train-validate split for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.data, data.target\n",
    "X_train_raw, X_valid_raw, y_train, y_valid = train_test_split(\n",
    "    X, y, train_size=0.7, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {X_train_raw.shape}\")\n",
    "print(f\"Validation data shape: {X_valid_raw.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Standardizing data using scikit-learn's `StandardScaler`. Scaling will improve the\n",
    " model accuracy, but I found it increased the runtime a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_scaling = True\n",
    "if use_scaling:\n",
    "    scaler = StandardScaler()  # There's more options than this one\n",
    "    scaler.fit(X_train_raw)\n",
    "    X_train = scaler.transform(X_train_raw)\n",
    "    X_valid = scaler.transform(X_valid_raw)\n",
    "else:\n",
    "    X_train = X_train_raw\n",
    "    X_valid = X_valid_raw\n",
    "\n",
    "# Convert to 2D PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).reshape(-1, 1)\n",
    "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
    "y_valid = torch.tensor(y_valid, dtype=torch.float32).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Defining our network\n",
    " Remember that we need the following items to train a neural network:\n",
    " - Data\n",
    " - A model\n",
    " - A loss function\n",
    " - An optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Defining the model\n",
    " We'll use a simple feed-forward neural network with 4 layers. I'm leaving these\n",
    " hardcoded, but notice that the input and output sizes are determined by the number of\n",
    " features (columns) in our X data and the number of feautures (columns) in our y data.\n",
    "\n",
    " The linear layers are the same matrix multiplication we saw in the slides. The ReLU is\n",
    " a commonly used activation function that keeps only positive values and introduces\n",
    " nonlinearity.  Since successive linear matrix multiplications can be represented with\n",
    " a single matrix, every intermediate (hidden) layer needs a nonlinear activation\n",
    " function.\n",
    " This network is using a structure where we decrease the number of \"neurons\" (weights)\n",
    " in each layer. This one is using a halving scheme, but you can try others. If we made\n",
    " this into a function, we could say how many layers we want, and how many neurons in\n",
    " the first layer, and try more options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=8, out_features=24),  # Z_i = W_ij * x_j + b_i\n",
    "    nn.ReLU(),  # keep only positive values, introduce nonlinearity\n",
    "    nn.Linear(in_features=24, out_features=12),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=12, out_features=6),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=6, out_features=1),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Loss function and optimizer\n",
    " We'll use mean squared error (MSE) as our loss function. This is a common loss\n",
    " function for regression tasks. We'll use the Adam optimizer which stands for Adaptive\n",
    " Moment Estimation. It's a common optimizer that uses adaptive learning rates and\n",
    " momentum (equation term for getting out of local minima) to speed up convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()  # mean square error\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Training\n",
    " We'll train for 4 epochs (passes through the data) with a batch size of 10. Training\n",
    " takes a while, so we'll use the `tqdm` library to show progress bars. We'll also keep\n",
    " track of the best model weights and return those at the end. We'll need two loops, one\n",
    " for each epoch and one for each batch.\n",
    " Reminder of what a training loop looks like:\n",
    "\n",
    " ![network-structure](../attachments/01-kg-regression/network-structure.png)\n",
    " \n",
    " Our model will already have the initial weights, so we don't need to initialize them\n",
    " manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 50  # number of epochs to train for\n",
    "batch_size = 10  # size of each batch\n",
    "batch_start = torch.arange(0, len(X_train), batch_size)  # start index of each batch\n",
    "\n",
    "# Hold the best model\n",
    "best_mse = np.inf  # init to infinity\n",
    "best_weights = None\n",
    "train_history = []\n",
    "valid_history = []\n",
    "\n",
    "with tqdm.notebook.trange(n_epochs, unit=\"epoch\") as bar:\n",
    "    bar.set_description(\"Epoch Progress\")\n",
    "    for epoch in bar:\n",
    "        model.train()\n",
    "        for start in batch_start:\n",
    "            # take a batch\n",
    "            X_batch = X_train[start : start + batch_size]\n",
    "            y_batch = y_train[start : start + batch_size]\n",
    "            # forward pass (calculating our model predictions)\n",
    "            y_valid_pred = model(X_batch)\n",
    "            loss = loss_fn(y_valid_pred, y_batch)\n",
    "            # backward pass (calculate gradients)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  # backward propagation\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "            # print progress\n",
    "            bar.set_postfix(mse=float(loss))\n",
    "\n",
    "        # evaluate our chosen loss metric at end of each epoch\n",
    "        model.eval()\n",
    "\n",
    "        # Evaluate the model on the training and validation data\n",
    "        y_train_pred = model(X_train)\n",
    "        train_mse = loss_fn(y_train_pred, y_train)\n",
    "        train_mse = float(train_mse)\n",
    "        train_history.append(train_mse)\n",
    "\n",
    "        y_valid_pred = model(X_valid)\n",
    "        valid_mse = loss_fn(y_valid_pred, y_valid)\n",
    "        valid_mse = float(valid_mse)\n",
    "        valid_history.append(valid_mse)\n",
    "\n",
    "        # Update the best weights\n",
    "        if valid_mse < best_mse:\n",
    "            best_mse = valid_mse\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Evaluation\n",
    " Let's plot our training history to see how our model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore model and return best version with the lowest validation loss\n",
    "model.load_state_dict(best_weights)\n",
    "print(\"MSE: %.2f\" % best_mse)\n",
    "print(\"RMSE: %.2f\" % np.sqrt(best_mse))\n",
    "# Overlay the training and validation loss history in a plot\n",
    "plt.plot(train_history, label=\"train\")\n",
    "plt.plot(valid_history, label=\"valid\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss: MSE\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():  # Reduce memory usage during inference\n",
    "    # Test out inference with 5 samples\n",
    "    for i in range(5):\n",
    "        X_sample = X_valid_raw[i : i + 1]\n",
    "        X_sample = scaler.transform(X_sample)\n",
    "        X_sample = torch.tensor(X_sample, dtype=torch.float32)\n",
    "        y_valid_pred = model(X_sample)\n",
    "        print(\n",
    "            f\"{X_valid_raw[i]} -> {y_valid_pred[0].numpy()} (expected {y_valid[i].numpy()})\"\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
